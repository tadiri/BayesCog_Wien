{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tadiri/BayesCog_Wien/blob/master/QLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "env = gym.make(\"FrozenLake-v1\",new_step_api=True)\n",
        "#env = gym.make(\"Taxi-v3\",new_step_api=True)\n",
        "n_observations = env.observation_space.n\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "#We start with a Q table of \"all zeros\"\n",
        "Q_table = np.zeros((n_observations,n_actions))\n",
        "print(Q_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fbmg-N6kVEK",
        "outputId": "900a530c-d536-4f62-b2ee-3d4b713681be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7n4xnE4H6AL",
        "outputId": "e0a262d4-8adc-4173-c0a2-ca941a4a0e0a"
      },
      "source": [
        "\n",
        "\n",
        "#the number of episodes we use for training\n",
        "n_episodes = 50000\n",
        "\n",
        "#maximum of transitions per episode\n",
        "max_traj_per_episode = 100\n",
        "\n",
        "#Epsilon-Greedy with start value 1\n",
        "epsilon = 1\n",
        "\n",
        "#value for decreasing Epsilon per step\n",
        "epsilon_decay = 0.0001\n",
        "\n",
        "# minimum of exploration proba\n",
        "min_epsilon = 0.001\n",
        "\n",
        "#discounted factor\n",
        "gamma = 1\n",
        "\n",
        "#learning rate\n",
        "lr = 0.0001\n",
        "\n",
        "total_rewards_episode = list()\n",
        "rewards_per_episode = list()\n",
        "\n",
        "\n",
        "# we train for a number of episodes\n",
        "for e in range(n_episodes):\n",
        "    #every time we start, we reset the environment\n",
        "    current_state = env.reset()\n",
        "    done = False\n",
        "\n",
        "    #we sum the individual rewards an agent gets per episode\n",
        "    total_episode_reward = 0\n",
        "\n",
        "    for i in range(max_traj_per_episode):\n",
        "        # Epsilon-Greedy calculation - do we explore, or exploit?\n",
        "        if np.random.uniform(0,1) < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            action = np.argmax(Q_table[current_state,:])\n",
        "\n",
        "        # we perform the selected action on the enviroment and get (1) the next state we are landing in, (2) the reward of the last action, (3) information if the episode ended\n",
        "        next_state, reward, done, truncated , info = env.step(action)\n",
        "        # observation, reward, done, info\n",
        "\n",
        "        # After each step, we update our Q-value for the last transition according to Q-learning\n",
        "        Q_table[current_state, action] = Q_table[current_state, action] +lr*(reward + gamma*max(Q_table[next_state,:])-Q_table[current_state, action])\n",
        "\n",
        "        # After each step, we add the reward of the last action (if any) to our episode reward\n",
        "        total_episode_reward = total_episode_reward + reward\n",
        "\n",
        "        # If the last action yieleded to the end of an episode, we stop and continue with a nuew run\n",
        "        if done:\n",
        "            break\n",
        "        current_state = next_state\n",
        "    #We reduce epsilon to slowly move from exploration to exploitation\n",
        "    epsilon = max(min_epsilon, np.exp(-epsilon_decay*e))\n",
        "    #At the end of every episode, we store the cummulative reward that we got\n",
        "    rewards_per_episode.append(total_episode_reward)\n",
        "\n",
        "    if(e%1000==0):\n",
        "      print(epsilon)\n",
        "\n",
        "\n",
        "\n",
        "#at the end of training, we also show how our Q-values have been updated\n",
        "print(Q_table)\n",
        "\n",
        "#We also show in steps of of 1000 if and how the average reward has been increased\n",
        "print(\"Mean rewards\")\n",
        "for i in range(n_episodes//1000):\n",
        "    print((i+1)*1000,\": mean espiode reward: \", np.mean(rewards_per_episode[1000*i:1000*(i+1)]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n",
            "0.9048374180359595\n",
            "0.8187307530779818\n",
            "0.7408182206817179\n",
            "0.6703200460356393\n",
            "0.6065306597126334\n",
            "0.5488116360940264\n",
            "0.49658530379140947\n",
            "0.44932896411722156\n",
            "0.4065696597405991\n",
            "0.36787944117144233\n",
            "0.33287108369807955\n",
            "0.30119421191220214\n",
            "0.2725317930340126\n",
            "0.24659696394160643\n",
            "0.22313016014842982\n",
            "0.20189651799465538\n",
            "0.1826835240527346\n",
            "0.16529888822158653\n",
            "0.14956861922263504\n",
            "0.1353352832366127\n",
            "0.1224564282529819\n",
            "0.11080315836233387\n",
            "0.1002588437228037\n",
            "0.09071795328941251\n",
            "0.0820849986238988\n",
            "0.07427357821433388\n",
            "0.06720551273974976\n",
            "0.06081006262521795\n",
            "0.05502322005640721\n",
            "0.049787068367863944\n",
            "0.0450492023935578\n",
            "0.04076220397836621\n",
            "0.036883167401239994\n",
            "0.033373269960326066\n",
            "0.0301973834223185\n",
            "0.02732372244729256\n",
            "0.024723526470339388\n",
            "0.02237077185616559\n",
            "0.02024191144580438\n",
            "0.01831563888873418\n",
            "0.016572675401761237\n",
            "0.014995576820477703\n",
            "0.013568559012200934\n",
            "0.012277339903068436\n",
            "0.011108996538242306\n",
            "0.010051835744633576\n",
            "0.009095277101695816\n",
            "0.00822974704902003\n",
            "0.007446583070924338\n",
            "[[5.85488844e-02 7.87929174e-02 5.20761363e-02 4.61554509e-02]\n",
            " [1.59108840e-02 5.19816188e-02 1.56955332e-02 2.59434251e-02]\n",
            " [7.79728130e-02 1.37236480e-02 1.71555972e-02 1.14397071e-02]\n",
            " [7.71400803e-03 1.11721502e-03 1.28609603e-04 1.13701826e-03]\n",
            " [1.06679445e-01 3.87524946e-02 3.36298552e-02 2.78430817e-02]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [1.57016268e-02 1.32146720e-02 1.06292988e-01 2.51853549e-03]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [2.17106580e-02 3.08557778e-02 2.56124560e-02 1.35820882e-01]\n",
            " [1.88148990e-02 3.23472631e-02 1.67049416e-01 1.77971169e-02]\n",
            " [3.96513970e-02 3.81527646e-02 2.41543348e-01 7.60996769e-03]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [1.10085190e-02 3.35131005e-02 3.64263183e-02 2.59788778e-01]\n",
            " [3.83955853e-02 6.22253970e-01 1.34121211e-01 1.03276532e-01]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
            "Mean rewards\n",
            "1000 : mean espiode reward:  0.01\n",
            "2000 : mean espiode reward:  0.014\n",
            "3000 : mean espiode reward:  0.017\n",
            "4000 : mean espiode reward:  0.025\n",
            "5000 : mean espiode reward:  0.028\n",
            "6000 : mean espiode reward:  0.022\n",
            "7000 : mean espiode reward:  0.036\n",
            "8000 : mean espiode reward:  0.025\n",
            "9000 : mean espiode reward:  0.04\n",
            "10000 : mean espiode reward:  0.039\n",
            "11000 : mean espiode reward:  0.06\n",
            "12000 : mean espiode reward:  0.048\n",
            "13000 : mean espiode reward:  0.054\n",
            "14000 : mean espiode reward:  0.043\n",
            "15000 : mean espiode reward:  0.054\n",
            "16000 : mean espiode reward:  0.06\n",
            "17000 : mean espiode reward:  0.066\n",
            "18000 : mean espiode reward:  0.055\n",
            "19000 : mean espiode reward:  0.061\n",
            "20000 : mean espiode reward:  0.052\n",
            "21000 : mean espiode reward:  0.069\n",
            "22000 : mean espiode reward:  0.071\n",
            "23000 : mean espiode reward:  0.077\n",
            "24000 : mean espiode reward:  0.06\n",
            "25000 : mean espiode reward:  0.081\n",
            "26000 : mean espiode reward:  0.078\n",
            "27000 : mean espiode reward:  0.073\n",
            "28000 : mean espiode reward:  0.078\n",
            "29000 : mean espiode reward:  0.085\n",
            "30000 : mean espiode reward:  0.07\n",
            "31000 : mean espiode reward:  0.084\n",
            "32000 : mean espiode reward:  0.07\n",
            "33000 : mean espiode reward:  0.078\n",
            "34000 : mean espiode reward:  0.089\n",
            "35000 : mean espiode reward:  0.086\n",
            "36000 : mean espiode reward:  0.093\n",
            "37000 : mean espiode reward:  0.071\n",
            "38000 : mean espiode reward:  0.069\n",
            "39000 : mean espiode reward:  0.067\n",
            "40000 : mean espiode reward:  0.081\n",
            "41000 : mean espiode reward:  0.083\n",
            "42000 : mean espiode reward:  0.073\n",
            "43000 : mean espiode reward:  0.073\n",
            "44000 : mean espiode reward:  0.077\n",
            "45000 : mean espiode reward:  0.084\n",
            "46000 : mean espiode reward:  0.083\n",
            "47000 : mean espiode reward:  0.088\n",
            "48000 : mean espiode reward:  0.087\n",
            "49000 : mean espiode reward:  0.08\n",
            "50000 : mean espiode reward:  0.079\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bO-vVlURq-ea"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}